Yes, makes total sense. Here's a simple diagnostic:
# %% Diagnostic: Check how partitions work

from pyspark.sql import functions as F

# Read ONLY 2025 partition
tactic_2025_only = spark.read.parquet(
    "/prod/sz/tsz/00150/cc/DTZTA_T_TACTIC_EVNT_HIST/EVNT_STRT_DT=2025*"
)

# Check min/max of EVNT_STRT_DT
print("=== 2025 Partition Only ===")
tactic_2025_only.select(
    F.min("EVNT_STRT_DT").alias("MIN_DATE"),
    F.max("EVNT_STRT_DT").alias("MAX_DATE"),
    F.countDistinct("EVNT_STRT_DT").alias("UNIQUE_DATES")
).show()

# See distribution of years in this partition
print("Year distribution in 2025 partition:")
tactic_2025_only.withColumn("YEAR", F.year("EVNT_STRT_DT")) \
    .groupBy("YEAR") \
    .count() \
    .orderBy("YEAR") \
    .show()
What this tells you:
If MIN_DATE is 2020-xx-xx → Partition is cumulative, contains all history. You only need latest partition.
If MIN_DATE is 2025-01-xx → Partition is year-specific. You need both 2024 and 2025 partitions.
Run this first, then we know how to proceed.