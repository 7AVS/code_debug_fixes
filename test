from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, countDistinct, lit

spark = SparkSession.builder \
    .appName("ODS Partition Diagnostic") \
    .getOrCreate()

# Read two specific partitions
df_jan20 = spark.read.parquet("/prod/01347/app/LS20/data/SparkJobData/effectDate=2026-01-20") \
    .withColumn("partition_date", lit("2026-01-20"))

df_jan19 = spark.read.parquet("/prod/01347/app/LS20/data/SparkJobData/effectDate=2026-01-19") \
    .withColumn("partition_date", lit("2026-01-19"))

# Count records in each
print("Records in 2026-01-20:", df_jan20.count())
print("Records in 2026-01-19:", df_jan19.count())

# Check overlap - do same CLNT_ID + OFFR_ID appear in both?
keys_jan20 = df_jan20.select("CLNT_ID", "OFFR_ID").distinct()
keys_jan19 = df_jan19.select("CLNT_ID", "OFFR_ID").distinct()

overlap = keys_jan20.intersect(keys_jan19).count()
only_in_jan20 = keys_jan20.subtract(keys_jan19).count()
only_in_jan19 = keys_jan19.subtract(keys_jan20).count()

print(f"\nOverlapping records (in both): {overlap}")
print(f"Only in 2026-01-20: {only_in_jan20}")
print(f"Only in 2026-01-19: {only_in_jan19}")

# Interpretation
if overlap > 0 and only_in_jan20 > 0:
    print("\n--> Looks like SNAPSHOT with incremental adds. Use latest partition.")
elif overlap == 0:
    print("\n--> Looks like APPEND. Use date range based on treatment window.")
else:
    print("\n--> Check results - may need more investigation.")
Run that and it'll tell you whether to use latest or a range.